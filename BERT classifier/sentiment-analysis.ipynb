{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7089488,"sourceType":"datasetVersion","datasetId":4085107}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-01T09:53:48.339640Z","iopub.execute_input":"2023-12-01T09:53:48.340074Z","iopub.status.idle":"2023-12-01T09:53:48.359969Z","shell.execute_reply.started":"2023-12-01T09:53:48.340031Z","shell.execute_reply":"2023-12-01T09:53:48.358534Z"},"trusted":true},"execution_count":215,"outputs":[{"name":"stdout","text":"/kaggle/input/harshdetection/sample.csv\n/kaggle/input/harshdetection/train.csv\n/kaggle/input/harshdetection/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the Data Files","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:48.361910Z","iopub.execute_input":"2023-12-01T09:53:48.362278Z","iopub.status.idle":"2023-12-01T09:53:48.366859Z","shell.execute_reply.started":"2023-12-01T09:53:48.362245Z","shell.execute_reply":"2023-12-01T09:53:48.365872Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"code","source":"main_df = pd.read_csv('/kaggle/input/harshdetection/train.csv')\ntest_df = pd.read_csv('/kaggle/input/harshdetection/test.csv')\nprint('Train Data Shape: ', main_df.shape)\nprint('Test Data Shape: ', test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:48.368096Z","iopub.execute_input":"2023-12-01T09:53:48.368452Z","iopub.status.idle":"2023-12-01T09:53:49.167366Z","shell.execute_reply.started":"2023-12-01T09:53:48.368422Z","shell.execute_reply":"2023-12-01T09:53:49.166430Z"},"trusted":true},"execution_count":217,"outputs":[{"name":"stdout","text":"Train Data Shape:  (89359, 8)\nTest Data Shape:  (38297, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"main_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.169299Z","iopub.execute_input":"2023-12-01T09:53:49.169610Z","iopub.status.idle":"2023-12-01T09:53:49.182397Z","shell.execute_reply.started":"2023-12-01T09:53:49.169583Z","shell.execute_reply":"2023-12-01T09:53:49.181423Z"},"trusted":true},"execution_count":218,"outputs":[{"execution_count":218,"output_type":"execute_result","data":{"text/plain":"                     id                                               text  \\\n0  a8be7c5d4527adbbf15f  \", 6 December 2007 (UTC)\\nI am interested, not...   \n1  0b7ca73f388222aad64d  I added about three missing parameters to temp...   \n2  db934381501872ba6f38  SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX   \n3  228015c4a87c4b1f09a7  why good sir? Why? \\n\\nYou, sir, obviously do ...   \n4  b18f26cfa1408b52e949  \"\\n\\n Source \\n\\nIncase I forget, or someone e...   \n\n   harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate  \n0      0                0       0            0           0              0  \n1      0                0       0            0           0              0  \n2      1                0       0            0           0              0  \n3      1                0       1            1           1              0  \n4      0                0       0            0           0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>harsh</th>\n      <th>extremely_harsh</th>\n      <th>vulgar</th>\n      <th>threatening</th>\n      <th>disrespect</th>\n      <th>targeted_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a8be7c5d4527adbbf15f</td>\n      <td>\", 6 December 2007 (UTC)\\nI am interested, not...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0b7ca73f388222aad64d</td>\n      <td>I added about three missing parameters to temp...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>db934381501872ba6f38</td>\n      <td>SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>228015c4a87c4b1f09a7</td>\n      <td>why good sir? Why? \\n\\nYou, sir, obviously do ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b18f26cfa1408b52e949</td>\n      <td>\"\\n\\n Source \\n\\nIncase I forget, or someone e...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.183563Z","iopub.execute_input":"2023-12-01T09:53:49.183857Z","iopub.status.idle":"2023-12-01T09:53:49.197998Z","shell.execute_reply.started":"2023-12-01T09:53:49.183832Z","shell.execute_reply":"2023-12-01T09:53:49.196963Z"},"trusted":true},"execution_count":219,"outputs":[{"execution_count":219,"output_type":"execute_result","data":{"text/plain":"                     id                                               text\n0  e0ae9d9474a5689a5791               in an interview before his execution\n1  b64a191301cad4f11287  He knew what he was doing. The below posts are...\n2  5e1953d9ae04bdc66408  Zzzzzzz... youre a real bore. Now go bore some...\n3  23128f98196c8e8f7b90  \"\\n\\nYet, it remains confusion because the 910...\n4  2d3f1254f71472bf2b78  I was referring to them losing interest in van...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e0ae9d9474a5689a5791</td>\n      <td>in an interview before his execution</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b64a191301cad4f11287</td>\n      <td>He knew what he was doing. The below posts are...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5e1953d9ae04bdc66408</td>\n      <td>Zzzzzzz... youre a real bore. Now go bore some...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23128f98196c8e8f7b90</td>\n      <td>\"\\n\\nYet, it remains confusion because the 910...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2d3f1254f71472bf2b78</td>\n      <td>I was referring to them losing interest in van...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"# Device\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.199454Z","iopub.execute_input":"2023-12-01T09:53:49.199831Z","iopub.status.idle":"2023-12-01T09:53:49.209708Z","shell.execute_reply.started":"2023-12-01T09:53:49.199799Z","shell.execute_reply":"2023-12-01T09:53:49.208623Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertInputItem(object):\n    \n    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n        self.text = text\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.211005Z","iopub.execute_input":"2023-12-01T09:53:49.211362Z","iopub.status.idle":"2023-12-01T09:53:49.220212Z","shell.execute_reply.started":"2023-12-01T09:53:49.211327Z","shell.execute_reply":"2023-12-01T09:53:49.219158Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"def formatInput(text, label, max_seq_length, tokenizer, verbose=0):\n    \n    bert_inputs = []\n    input_pair = zip(text, label)\n    \n    for(index, (text, label)) in tqdm(enumerate(input_pair)):\n        \n        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n        if len(input_ids) > max_seq_length:\n            input_ids = input_ids[:max_seq_length]\n            \n        segment_ids = [0] * len(input_ids)\n        input_mask = [1] * len(input_ids)\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n        \n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        \n        bert_inputs.append(BertInputItem(text=text,\n                                        input_ids = input_ids,\n                                        input_mask = input_mask,\n                                        segment_ids = segment_ids,\n                                        label_id = label))\n    \n    return bert_inputs","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.223482Z","iopub.execute_input":"2023-12-01T09:53:49.224122Z","iopub.status.idle":"2023-12-01T09:53:49.235681Z","shell.execute_reply.started":"2023-12-01T09:53:49.224084Z","shell.execute_reply":"2023-12-01T09:53:49.234733Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader):\n    model.eval()\n    \n    eval_loss = 0\n    nb_eval_steps = 0\n    predicted_labels, correct_labels = [], []\n\n    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n\n        with torch.no_grad():\n            tmp_eval_loss, logits = model(input_ids, attention_mask=input_mask,\n                                          token_type_ids=segment_ids, labels=label_ids)\n\n        outputs = np.argmax(logits.to('cpu'), axis=1)\n        label_ids = label_ids.to('cpu').numpy()\n        \n        predicted_labels += list(outputs)\n        correct_labels += list(label_ids)\n        \n        eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    \n    correct_labels = np.array(correct_labels)\n    predicted_labels = np.array(predicted_labels)\n        \n    return eval_loss, correct_labels, predicted_labels","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.237005Z","iopub.execute_input":"2023-12-01T09:53:49.237313Z","iopub.status.idle":"2023-12-01T09:53:49.248062Z","shell.execute_reply.started":"2023-12-01T09:53:49.237287Z","shell.execute_reply":"2023-12-01T09:53:49.247126Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n\ndef get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n\n    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.249366Z","iopub.execute_input":"2023-12-01T09:53:49.249805Z","iopub.status.idle":"2023-12-01T09:53:49.262197Z","shell.execute_reply.started":"2023-12-01T09:53:49.249774Z","shell.execute_reply":"2023-12-01T09:53:49.261351Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\nfrom transformers import BertTokenizer\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom tqdm import trange\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\n\nimport time\n\n\nMAX_SEQ_LENGTH = 100\nBATCH_SIZE = 16\nGRADIENT_ACCUMULATION_STEPS = 1\nNUM_TRAIN_EPOCHS = 20\nLEARNING_RATE = 5e-5\nWARMUP_PROPORTION = 0.1\nMAX_GRAD_NORM = 5\nOUTPUT_DIR = \"/tmp/\"\nMODEL_FILE_NAME = \"bert_model.pth\"\nPATIENCE = 2\nSTART_TIME = time.time()\n\ndef train(column, BERT_MODEL = 'bert-base-uncased'):\n    \n    model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n    model.to(device)\n    \n    print(f'1. Model Loaded. Time = {time.time()-START_TIME}')\n    \n    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n    \n    print(f'2. Tokenizer Loaded, Time = {time.time()-START_TIME}')\n    \n    X_main = main_df['text']\n    y_main = main_df[column]\n    \n    X_train, X_cv, y_train, y_cv = train_test_split(X_main, y_main, test_size=0.2, random_state=42)\n    \n    train_features = formatInput(X_train, y_train, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n    cv_features = formatInput(X_cv, y_cv, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n    \n    print(f'3. Data Formatted, Time = {time.time()-START_TIME}')\n    \n    train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n    cv_dataloader = get_data_loader(cv_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n    \n    print(f'4. Data Loaded, Time = {time.time()-START_TIME}')\n    \n    num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n    num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps = -1)    \n    \n    print(f\"Training Starts for {NUM_TRAIN_EPOCHS} epochs\")\n    \n    loss_history = []\n    no_improvement = 0\n    for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n        model.train()\n        tr_loss = 0\n        nb_tr_examples, nb_tr_steps = 0, 0\n        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n\n            outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n            loss = outputs[0]\n\n            if GRADIENT_ACCUMULATION_STEPS > 1:\n                loss = loss / GRADIENT_ACCUMULATION_STEPS\n\n            loss.backward()\n            tr_loss += loss.item()\n\n            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)  \n\n                optimizer.step()\n                optimizer.zero_grad()\n                scheduler.step()\n\n        dev_loss, _, _ = evaluate(model, dev_dataloader)\n\n        print(\"Loss history:\", loss_history)\n        print(\"Dev loss:\", dev_loss)\n\n        if len(loss_history) == 0 or dev_loss < min(loss_history):\n            no_improvement = 0\n            model_to_save = model.module if hasattr(model, 'module') else model\n            output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n            torch.save(model_to_save.state_dict(), output_model_file)\n        else:\n            no_improvement += 1\n\n        if no_improvement >= PATIENCE: \n            print(\"No improvement on development set. Finish training.\")\n            break\n\n\n        loss_history.append(dev_loss)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.263417Z","iopub.execute_input":"2023-12-01T09:53:49.263706Z","iopub.status.idle":"2023-12-01T09:53:49.285450Z","shell.execute_reply.started":"2023-12-01T09:53:49.263681Z","shell.execute_reply":"2023-12-01T09:53:49.284424Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"main_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.286803Z","iopub.execute_input":"2023-12-01T09:53:49.287301Z","iopub.status.idle":"2023-12-01T09:53:49.308103Z","shell.execute_reply.started":"2023-12-01T09:53:49.287262Z","shell.execute_reply":"2023-12-01T09:53:49.307082Z"},"trusted":true},"execution_count":226,"outputs":[{"execution_count":226,"output_type":"execute_result","data":{"text/plain":"                     id                                               text  \\\n0  a8be7c5d4527adbbf15f  \", 6 December 2007 (UTC)\\nI am interested, not...   \n1  0b7ca73f388222aad64d  I added about three missing parameters to temp...   \n2  db934381501872ba6f38  SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX   \n3  228015c4a87c4b1f09a7  why good sir? Why? \\n\\nYou, sir, obviously do ...   \n4  b18f26cfa1408b52e949  \"\\n\\n Source \\n\\nIncase I forget, or someone e...   \n\n   harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate  \n0      0                0       0            0           0              0  \n1      0                0       0            0           0              0  \n2      1                0       0            0           0              0  \n3      1                0       1            1           1              0  \n4      0                0       0            0           0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>harsh</th>\n      <th>extremely_harsh</th>\n      <th>vulgar</th>\n      <th>threatening</th>\n      <th>disrespect</th>\n      <th>targeted_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a8be7c5d4527adbbf15f</td>\n      <td>\", 6 December 2007 (UTC)\\nI am interested, not...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0b7ca73f388222aad64d</td>\n      <td>I added about three missing parameters to temp...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>db934381501872ba6f38</td>\n      <td>SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>228015c4a87c4b1f09a7</td>\n      <td>why good sir? Why? \\n\\nYou, sir, obviously do ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b18f26cfa1408b52e949</td>\n      <td>\"\\n\\n Source \\n\\nIncase I forget, or someone e...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train(column = 'harsh')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.309605Z","iopub.execute_input":"2023-12-01T09:53:49.310300Z","iopub.status.idle":"2023-12-01T09:53:49.318887Z","shell.execute_reply.started":"2023-12-01T09:53:49.310268Z","shell.execute_reply":"2023-12-01T09:53:49.317854Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\nmodel.to(device)\n\nprint(f'1. Model Loaded. Time = {time.time()-START_TIME}')\n\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n\nprint(f'2. Tokenizer Loaded, Time = {time.time()-START_TIME}')\n\nX_main = main_df['text']\ny_main = main_df['harsh']\n\nX_train, X_cv, y_train, y_cv = train_test_split(X_main, y_main, test_size=0.2, random_state=42)\n\ntrain_features = formatInput(X_train, y_train, MAX_SEQ_LENGTH, tokenizer, verbose=0)\ncv_features = formatInput(X_cv, y_cv, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n\nprint(f'3. Data Formatted, Time = {time.time()-START_TIME}')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T09:53:49.320376Z","iopub.execute_input":"2023-12-01T09:53:49.320747Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1. Model Loaded. Time = 1.573256015777588\n2. Tokenizer Loaded, Time = 1.7353920936584473\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/3579748240.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for(index, (text, label)) in tqdm(enumerate(input_pair)):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e208d2674f143e7af5fecc7b8ebae31"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\ncv_dataloader = get_data_loader(cv_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n\nprint(f'4. Data Loaded, Time = {time.time()-START_TIME}')\n\nnum_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps = -1)    \n\nprint(f\"Training Starts for {NUM_TRAIN_EPOCHS} epochs\")\n\nloss_history = []\nno_improvement = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n    model.train()\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n\n        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n        loss = outputs[0]\n\n        if GRADIENT_ACCUMULATION_STEPS > 1:\n            loss = loss / GRADIENT_ACCUMULATION_STEPS\n\n        loss.backward()\n        tr_loss += loss.item()\n\n        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)  \n\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n\n    cv_loss, _, _ = evaluate(model, cv_dataloader)\n\n    print(\"Loss history:\", loss_history)\n    print(\"Cv loss:\", cv_loss)\n\n    if len(loss_history) == 0 or cv_loss < min(loss_history):\n        no_improvement = 0\n        model_to_save = model.module if hasattr(model, 'module') else model\n        output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n        torch.save(model_to_save.state_dict(), output_model_file)\n    else:\n        no_improvement += 1\n\n    if no_improvement >= PATIENCE: \n        print(\"No improvement on cv set. Finish training.\")\n        break\n\n\n    loss_history.append(cv_loss)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in trange(100,desc=\"COVAXIN\",unit=\"KAGGLE\"):\n    sleep(0.01)","metadata":{},"execution_count":null,"outputs":[]}]}